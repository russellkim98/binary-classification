{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38154f1b-d91b-4199-99dd-1ce1f279ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/russellkim-2023-intel/Library/Caches/pypoetry/virtualenvs/stochastic-decision-modeling-pvQtmOvi-py3.11/lib/python3.11/site-packages/tensorflow/python/client/session.py:1793: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pprint import pprint\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "import inspect\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.drivers import driver\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "# Imports for example.\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\n",
    "from tf_agents.bandits.metrics import tf_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "nest = tf.nest\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a9607-7a97-4aec-a54c-4a5ee4e12225",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006dd093-a7fa-4a76-a8c4-e33ae7bc6320",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 2 # @param\n",
    "arm0_param = [-3, 0, 1, -2] # @param\n",
    "arm1_param = [1, -2, 3, 0] # @param\n",
    "arm2_param = [0, 0, 1, 1] # @param\n",
    "def context_sampling_fn(batch_size):\n",
    "  \"\"\"Contexts from [-10, 10]^4.\"\"\"\n",
    "  def _context_sampling_fn():\n",
    "    return np.random.randint(-10, 10, [batch_size, 4]).astype(np.float32)\n",
    "  return _context_sampling_fn\n",
    "\n",
    "class LinearNormalReward(object):\n",
    "  \"\"\"A class that acts as linear reward function when called.\"\"\"\n",
    "  def __init__(self, theta, sigma):\n",
    "    self.theta = theta\n",
    "    self.sigma = sigma\n",
    "  def __call__(self, x):\n",
    "    mu = np.dot(x, self.theta)\n",
    "    return np.random.normal(mu, self.sigma)\n",
    "\n",
    "arm0_reward_fn = LinearNormalReward(arm0_param, 1)\n",
    "arm1_reward_fn = LinearNormalReward(arm1_param, 1)\n",
    "arm2_reward_fn = LinearNormalReward(arm2_param, 1)\n",
    "\n",
    "environment = tf_py_environment.TFPyEnvironment(\n",
    "    sspe.StationaryStochasticPyEnvironment(\n",
    "        context_sampling_fn(batch_size),\n",
    "        [arm0_reward_fn, arm1_reward_fn, arm2_reward_fn],\n",
    "        batch_size=batch_size))\n",
    "\n",
    "\n",
    "observation_spec = tensor_spec.TensorSpec([4], tf.float32)\n",
    "time_step_spec = ts.time_step_spec(observation_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    dtype=tf.int32, shape=(), minimum=0, maximum=2)\n",
    "\n",
    "agent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n",
    "                                     action_spec=action_spec)\n",
    "\n",
    "def compute_optimal_reward(observation):\n",
    "  expected_reward_for_arms = [\n",
    "      tf.linalg.matvec(observation, tf.cast(arm0_param, dtype=tf.float32)),\n",
    "      tf.linalg.matvec(observation, tf.cast(arm1_param, dtype=tf.float32)),\n",
    "      tf.linalg.matvec(observation, tf.cast(arm2_param, dtype=tf.float32))]\n",
    "  optimal_action_reward = tf.reduce_max(expected_reward_for_arms, axis=0)\n",
    "  return optimal_action_reward\n",
    "\n",
    "regret_metric = tf_metrics.RegretMetric(compute_optimal_reward)\n",
    "\n",
    "num_iterations = 90 # @param\n",
    "steps_per_loop = 1 # @param\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=steps_per_loop)\n",
    "\n",
    "observers = [replay_buffer.add_batch, regret_metric]\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=environment,\n",
    "    policy=agent.collect_policy,\n",
    "    num_steps=steps_per_loop * batch_size,\n",
    "    observers=observers)\n",
    "\n",
    "regret_values = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  driver.run()\n",
    "  loss_info = agent.train(replay_buffer.gather_all())\n",
    "  replay_buffer.clear()\n",
    "  regret_values.append(regret_metric.result())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa860d-d1ef-4c2e-a0b2-efa59a96ed9d",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca31989c-b6b9-4d53-a9b1-e1b841eac45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.agents import bernoulli_thompson_sampling_agent as bern_ts_agent\n",
    "\n",
    "agent = bern_ts_agent.BernoulliThompsonSamplingAgent(\n",
    "    time_step_spec=environment.time_step_spec(),\n",
    "    action_spec=environment.action_spec(),\n",
    "    dtype=tf.float64,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115039a-e528-4bd4-bee1-cbb025a5da38",
   "metadata": {},
   "source": [
    "## Observers (Reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387656ae-65e9-4ee1-ada8-0e596906d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2, 0.3, 0.45, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(environment.pyenv._means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449fc058-9d05-4320-9e18-672dfaafd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "\n",
    "def optimal_reward_fn(unused_observation):\n",
    "    return np.max(environment.pyenv._means)\n",
    "\n",
    "def optimal_action_fn(unused_observation):\n",
    "    return np.int32(np.argmax(environment.pyenv._means))\n",
    "\n",
    "\n",
    "observers = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size),\n",
    "    tf_metrics.AverageReturnMetric(batch_size=environment.batch_size),\n",
    "    tf_bandit_metrics.RegretMetric(optimal_reward_fn),\n",
    "    tf_bandit_metrics.SuboptimalArmsMetric(optimal_action_fn)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c7999-5b71-490a-9e5b-902f33c3b885",
   "metadata": {},
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea3ebe9-150b-4d7a-859e-acee8e35e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.drivers import dynamic_step_driver\n",
    "steps_per_loop = 1\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "  env=environment,\n",
    "  policy=agent.collect_policy,\n",
    "  num_steps=steps_per_loop * environment.batch_size,\n",
    "  observers=observers,\n",
    ")\n",
    "data_spec = agent.policy.trajectory_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e18e30cb-9acb-4382-bc47-a34453717495",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3215892952.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pprint(driver.env.)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pprint(driver.env.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d339c6-ee40-485f-a0c0-e09ad68dd865",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9f672d1-6cb3-473a-937f-0fe1d806a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.replay_buffers import bandit_replay_buffer\n",
    "replay_buffer = bandit_replay_buffer.BanditReplayBuffer(\n",
    "      data_spec=data_spec,\n",
    "      batch_size=batch_size,\n",
    "      max_length=steps_per_loop\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c83eda-125c-49f5-88f7-9d5149e394ad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2cd84e-a49b-49e7-8bd9-2747e78ec140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import export_utils\n",
    "from io import StringIO\n",
    "import logging\n",
    "log_stream = StringIO()    \n",
    "logging.basicConfig(stream=log_stream, level=logging.NOTSET)\n",
    "\n",
    "def _export_metrics_and_summaries(step, metrics):\n",
    "    \"\"\"Exports metrics and tf summaries.\"\"\"\n",
    "    metric_utils.log_metrics(metrics)\n",
    "    export_utils.export_metrics(step=step, metrics=metrics)\n",
    "    for metric in metrics:\n",
    "      metric.tf_summaries(train_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed11ed71-8e2b-4301-b5d8-133e04b29a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_loop = 0\n",
    "training_loops = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70524207-e566-4341-bd68-0d39aa8b76bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TFStepMetric._update_state at 0x13ee6f380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "driver.run()\n",
    "dataset_it = iter(\n",
    "    replay_buffer.as_dataset(\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=100,\n",
    "        single_deterministic_pass=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ca01d28-d9b7-4aa7-acae-91ce784e7c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.observers[1].tf_summaries(train_step=0)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "220253d2-6d20-423b-a247-8cc211d718af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.StringIO at 0x142ffa050>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce0dd6-513a-47ce-bf4e-0dbdecd6c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_step, metrics):\n",
    "    \"\"\"Returns a function that runs a single training loop and logs metrics.\"\"\"\n",
    "    driver.run()\n",
    "    _export_metrics_and_summaries(\n",
    "      step=train_step, metrics=metrics\n",
    "    )\n",
    "    batch_size = driver.env.batch_size\n",
    "    dataset_it = iter(\n",
    "        replay_buffer.as_dataset(\n",
    "            sample_batch_size=batch_size,\n",
    "            num_steps=100,\n",
    "            single_deterministic_pass=True,\n",
    "        )\n",
    "    )\n",
    "    experience, unused_buffer_info = dataset_it.get_next()\n",
    "    set_expected_shape(experience, steps)\n",
    "    loss_info = agent.train(experience)\n",
    "    export_utils.export_metrics(\n",
    "      step=train_step * async_steps_per_loop + batch_id,\n",
    "      metrics=[],\n",
    "      loss_info=loss_info,\n",
    "    )\n",
    "    \n",
    "    replay_buffer.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
